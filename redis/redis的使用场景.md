在当前互联网形式下，redis会被应用到各种业务场景，多样的数据结构，会提高开发中对业务功能的开发效率。下面我们就来介绍下，针对不同的业务场景，redis是怎么帮助我们完成“数据统计”和数据计算的。

### 聚合统计

聚合统计：所谓的聚合统计，就是指统计多个集合元素的聚合结果，包括：统计多个集合的共有元素（交集统计）；把两个集合相比，统计其中一个集合独有的元素（差集统计）；统计多个集合的所有元素（并集统计）。

redis的set数据类型就可以帮助我们完成这些计算。下面我们举个例子来理解下：
```
Redis 集合(Set)
Redis 的 Set 是 String 类型的无序集合。集合成员是唯一的，这就意味着集合中不能出现重复的数据。

集合对象的编码可以是 intset 或者 hashtable。

Redis 中集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。

集合中最大的成员数为 2^32 - 1 (4294967295, 每个集合可存储40多亿个成员)。
```

假如你要统计app的每天的用户新增数和第二天的用户留存数，就正好对应到了聚合统计上。我们可以这么设计

1.user:id作为key，set中的值都是每天登录过app的用户id。这个集合保存的是登录过的所有用户的数据。每次新增一个用户我们都可以使用sadd命令加入集合中
```redis
sadd user:id 1000 10001 10002
```

2.user:id:day作为key，set中的值保存当天登录的用户id
```redis
sadd user:id:2021-12-09 1001 1002 1003
```

每天的用户新增数计算就可以使用下面的命令
```redis
SDIFFSTORE user:new user:id:20200804 user:id:20200803 
```
SDIFFSTORE命令会求出user:id:20200804 user:id:20200803 这两个集合的差集，然后将结果保存在user:new集合中，此时user:new就是我们需要的数据

第二天的用户留存数
```redis
SINTERSTORE user:save user:id:20200804 user:id:20200803 
```
SINTERSTORE命令会求出user:id:20200804 user:id:20200803 这两个集合的交集，然后将结果保存在user:save集合中，此时user:save就是我们需要的数据

当你需要对多个集合进行聚合计算时，Set 类型会是一个非常不错的选择。
不过需要注意的是Set 的差集、并集和交集的计算复杂度较高，在数据量较大的情况下，如果直接执行这些计算，会导致 Redis 实例阻塞。
所以，我给你分享一个小建议：你可以从主从集群中选择一个从库，让它专门负责聚合计算，或者是把数据读取到客户端，在客户端来完成聚合统计，这样就可以规避阻塞主库实例和其他从库实例的风险了。

还需要注意的是上面的SDIFFSTORE和SINTERSTORE命令都会在生成一个新的set集合，如果在从库执行的话，而从库一般是readonly的(当然从库也可以设置写入，但是一般不会这么做)，
所以这些命令只能在主库使用。想在从库上操作，可以使用SUNION、SDIFF、SINTER，这些命令可以计算出结果，但不会生成新key。

### 排序统计
下面我们以电商网站评论这个需求，来分析下redis应该如何做？

评论需求一般是按照评论的时间序列排序的，最新的评论应该在最前面，而且还得支持分页需求。redis数据结构里面，list和zset是可以支持顺序的。
list：根据放入数据的前后顺序排序
zset：根据设定的score权重来排序
看起来这两种都支持我们的评论获取需求，但是哪个更合适呢，我们分析下。

List
首先我们采用list的话会怎么设计呢？如果新增一条记录我们就是用lpush命令，将新的记录插入到list的头部。取数据的时候lrange key page*N page*N+N (N==每页数量)如图
```
list : {A, B, C, D, E}
==>插入F  lpush key value
list : {F, A, B, C, D, E}

==>取数据 page=1
list : {F, A, B}
==>取数据 page=2
list : {C, D, E}

list : {G, A, B, C, D, E, F}
```
看起来好像没有问题呢。假如此时回去第二页的时候，又新插入了数据H，此时分页获取的又会怎样呢？可以看到下面的实例插入一个H的时候，page=2时又获取到了数据B，
你可能会想，那有什么关系呢，客户端去个重机就可以了呀，但是如果插入的TPS很多呢，那么客户端就有可能取不到新的数据了。这种情况感觉是也是有问题呢。所以pass掉。
```
list : {G, A, B, C, D, E, F}

==>取数据 page=1
list : {G, A, B}

==>插入F  lpush key value
list : {H, F, A, B, C, D, E}

==>取数据 page=2，重复取到了数据B，page=1时已经获取到了。
list : {B, C, D}
```


zset就不会存在list的问题，因为它是根据元素的实际权重来排序和获取数据的。下面我们看看zset会怎么处理呢？
score代表时间戳，可以根据业务需求精确，插入一条数据时zadd key score value。我们可以通过下面命令获取到排序后的结果
假设越新的评论权重越大，目前最新评论的权重是 N，我们执行下面的命令时，就可以获得最新的 10 条评论：
```
ZRANGEBYSCORE comments N-9 N
```

二值状态统计
所谓的二值状态统计，就是记录用户此刻的0、1状态。比如现实生活中签到的任务。
我们可以使用redis里面的Bitmap结构，Bitmap 本身是用 String 类型作为底层数据结构实现的一种统计二值状态的数据类型。
String 类型是会保存为二进制的字节数组，所以，Redis 就把字节数组的每个 bit 位利用起来，用来表示一个元素的二值状态。你可以把 Bitmap 看作是一个 bit 数组。

假如我们要记录用户在2021-12-12的签到，bitmap是从0开始的，所以value=11
```
SETBIT uid:sign:3000:202012 11 1 
```

检查用户在2021-12-12是否签到
```
GETBIT uid:sign:3000:202012 11
```

统计用户在12月份的签到记录
```
BITCOUNT uid:sign:3000:202012
```

再举个例子，假如你要统计1亿用户在十天的签到记录，又该怎么设计呢。我们将日期作为key，bit当做十亿用户的签到状态，是可以包含10亿个bit位的。(bitmap可以设置最长多长呢，可以看后面的彩蛋)

命令 BITOP operation destkey key [key ...]
说明：对一个或多个保存二进制位的字符串 key 进行位元操作，并将结果保存到 destkey 上。
说明：BITOP 命令支持 AND 、 OR 、 NOT 、 XOR 这四种操作中的任意一种参数

我们可以使用BITOP的异或 或操作得出想要的数据。
如果是求1亿用户在10天内登录过的用户，可以使用XOR异或10天的key

内存计算：我们可以计算一下记录了10天签到情况后的内存开销。每天使用1个1亿位的Bitmap，大约占12MB的内存（10^8/8/1024/1024），
10天的Bitmap的内存开销约为120MB，内存压力不算太大。不过，在实际应用时，最好对Bitmap设置过期时间，让Redis自动删除不再需要的签到记录，以节省内存开销。

彩蛋：
实际上, redis只支持5种数据类型. 并没有bitmap. 也就是bitmap是基于redis的字符串类型的. 而一个字符串类型最多存储512M.
首先: 计算机的单位换算先了解下：

 8 bit = 1byte  

 1024 byte = 1kb

 1024 kb = 1Mb

其次:
我们使用的bitmap指令SETBIT  key  offset value, 这个指令就是将第offset设置成0或1.  比如 SETBIT  ss  1000 1 //就是将1000位置为1.  
1 bit就是1位,  所以我们只要将512M换算成bit, 那么就知道bitmap支持的最大设置长度了.  计算如下
8 * 1024 * 1024 * 512  =  2^32 (所以这个结果就是这么来的) 


### 基数统计
基数统计：就是计算一个集合中不重复元素的个数。最多的使用场景就是统计网页的UV，用户访问次数，同一个用户多次访问只能算一次。

看到这个定义你首先想到的是hash、set这些可以自动帮你去重的数据结构。可是这些结构都有个弊端，数量大的时候，会存在大key。也会消耗很大的内存。
那么什么结构既可以有hash、set的去重，又能节省内存呢，。

hyperLogog：HyperLogLog 是一种用于统计基数的数据集合类型，它的最大优势就在于，当集合元素数量非常多时，它计算基数所需的空间总是固定的，而且还很小。
在redis中，每个HyperLogLog只占用12k的内存，就能计算2^64的元素的基数。你可以使用如下命令来添加元素
```
PFADD page1:uv user1 user2 user3 user4 user5
```

然后使用如下命令来统计数量
```
PFCOUNT page1:uv
```

hyperloglog的简单解释：
hyperloglog是用户基数统计的数据结构，所以不会像hash，set返回实际存储的数据。这是需要重点关注的。
```
PFADD younger mysql redis pika es zk zk
//结果是5，zk重复
PFCOUNT younger

PFADD younger2 mysql c++ golang c
//结果是4
PFCOUNT younger2

//返回两个集合的交集的结果相加值
//结果是8不是9， 因为mysql数据重复了。
PFCOUNT younger1 younger2

//创建一个新对象new， 将younger1 younger2的值存储到new
PFMERGE new younger1 younger2

//结果为7
PFCOUNT new
```

不过，有一点需要你注意一下，HyperLogLog 的统计规则是基于概率完成的，所以它给出的统计结果是有一定误差的，标准误算率是 0.81%。
这也就意味着，你使用 HyperLogLog 统计的 UV 是 100 万，但实际的 UV 可能是 101 万。虽然误差率不算大，但是，如果你需要精确统计结果的话，最好还是继续用 Set 或 Hash 类型。
![redis使用场景](https://static001.geekbang.org/resource/image/c0/6e/c0bb35d0d91a62ef4ca1bd939a9b136e.jpg)
